
TODO
- convert datatypes where necessary
- convert Index into external create index statements
- many other things can be preserved


postgres:

 CONSTRAINT constraint_name ]
{ CHECK ( expression ) |
  UNIQUE ( column_name [, ... ] ) index_parameters |
  PRIMARY KEY ( column_name [, ... ] ) index_parameters |
  EXCLUDE [ USING index_method ] ( exclude_element WITH operator [, ... ] ) index_parameters [ WHERE ( predicate ) ] |
  FOREIGN KEY ( column_name [, ... ] ) REFERENCES reftable [ ( refcolumn [, ... ] ) ]
    [ MATCH FULL | MATCH PARTIAL | MATCH SIMPLE ] [ ON DELETE action ] [ ON UPDATE action ] }
[ DEFERRABLE | NOT DEFERRABLE ] [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ]


  postgres: FOREIGN KEY ( column_name [, ... ] ) REFERENCES reftable [ ( refcolumn [, ... ] ) ]
            [ MATCH FULL | MATCH PARTIAL | MATCH SIMPLE ] [ ON DELETE action ] [ ON UPDATE action ] }
  mysql:    CONSTRAINT `notes_ibfk_1` FOREIGN KEY (`submitter_id`) REFERENCES `users` (`id`) ON DELETE CASCADE

    REFERENCES tbl_name (index_col_name,...)
      [MATCH FULL | MATCH PARTIAL | MATCH SIMPLE]
      [ON DELETE reference_option]
      [ON UPDATE reference_option]





MySQL                   PostgreSQL          SQLite

TINYINT                 SMALLINT            INTEGER
SMALLINT                SMALLINT
MEDIUMINT               INTEGER
BIGINT                  BIGINT
BIT                     BIT                 INTEGER
_______________________________________________________

TINYINT UNSIGNED        SMALLINT            INTEGER
SMALLINT UNSIGNED       INTEGER
MEDIUMINT UNSIGNED      INTEGER
INT UNSIGNED            BIGINT
BIGINT UNSIGNED         NUMERIC(20)
_______________________________________________________

DOUBLE                  DOUBLE PRECISION    REAL
FLOAT                   REAL                REAL
DECIMAL                 DECIMAL             REAL
NUMERIC                 NUMERIC             REAL
_______________________________________________________

BOOLEAN                 BOOLEAN             INTEGER
_______________________________________________________

DATE                    DATE                TEXT
TIME                    TIME
DATETIME                TIMESTAMP
_______________________________________________________

TIMESTAMP DEFAULT       TIMESTAMP DEFAULT   TEXT
NOW()                   NOW()   
_______________________________________________________

LONGTEXT                TEXT                TEXT
MEDIUMTEXT              TEXT                TEXT
BLOB                    BYTEA               BLOB
VARCHAR                 VARCHAR             TEXT
CHAR                    CHAR                TEXT
_______________________________________________________

columnname INT          columnname SERIAL   INTEGER PRIMARY 
AUTO_INCREMENT                              KEY AUTOINCREMENT




------------------------------------------------------------------------




getTables :: conn -> IO [String]Source

The names of all tables accessible by the current connection, excluding special meta-tables (system tables).

You should expect this to be returned in the same manner as a result from fetchAllRows'.

All results should be converted to lowercase for you before you see them.


describeTable :: conn -> String -> IO [(String, SqlColDesc)]Source

Obtain information about the columns in a specific table. The String in the result set is the column name.

You should expect this to be returned in the same manner as a result from fetchAllRows'.

All results should be converted to lowercase for you before you see them. 


TODO need to deal with MySQL table encodings

) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=latin1;

convert.hs: SqlError {seState = "22021", seNativeError = 7, seErrorMsg =
"execute: PGRES_FATAL_ERROR: ERROR:  invalid byte sequence for encoding
\"UTF8\": 0xe2e3cf\n"}


See this:
http://dev.mysql.com/doc/refman/5.1/en/alter-table.html

    To change the table default character set and all character columns (CHAR, VARCHAR, TEXT) to a new character set, use a statement like this:

    ALTER TABLE tbl_name CONVERT TO CHARACTER SET charset_name;

    For a column that has a data type of VARCHAR or one of the TEXT types, CONVERT TO CHARACTER SET will change the data type as necessary to ensure that the new column is long enough to store as many characters as the original column. For example, a TEXT column has two length bytes, which store the byte-length of values in the column, up to a maximum of 65,535. For a latin1 TEXT column, each character requires a single byte, so the column can store up to 65,535 characters. If the column is converted to utf8, each character might require up to three bytes, for a maximum possible length of 3 × 65,535 = 196,605 bytes. That length will not fit in a TEXT column's length bytes, so MySQL will convert the data type to MEDIUMTEXT, which is the smallest string type for which the length bytes can record a value of 196,605. Similarly, a VARCHAR column might be converted to MEDIUMTEXT.

    To avoid data type changes of the type just described, do not use CONVERT TO CHARACTER SET. Instead, use MODIFY to change individual columns. For example:

    ALTER TABLE t MODIFY latin1_text_col TEXT CHARACTER SET utf8;
    ALTER TABLE t MODIFY latin1_varchar_col VARCHAR(M) CHARACTER SET utf8;

    If you specify CONVERT TO CHARACTER SET binary, the CHAR, VARCHAR, and TEXT columns are converted to their corresponding binary string types (BINARY, VARBINARY, BLOB). This means that the columns no longer will have a character set and a subsequent CONVERT TO operation will not apply to them.

    If charset_name is DEFAULT, the database character set is used.
    Warning

    The CONVERT TO operation converts column values between the character sets. This is not what you want if you have a column in one character set (like latin1) but the stored values actually use some other, incompatible character set (like utf8). In this case, you have to do the following for each such column:

    ALTER TABLE t1 CHANGE c1 c1 BLOB;
    ALTER TABLE t1 CHANGE c1 c1 TEXT CHARACTER SET utf8;

    The reason this works is that there is no conversion when you convert to or from BLOB columns.

    To change only the default character set for a table, use this statement:

    ALTER TABLE tbl_name DEFAULT CHARACTER SET charset_name;

    The word DEFAULT is optional. The default character set is the character set that is used if you do not specify the character set for columns that you add to a table later (for example, with ALTER TABLE ... ADD column). 

With the mysql_info() C API function, you can find out how many rows were copied by ALTER TABLE, and (when IGNORE is used) how many rows were deleted due to duplication of unique key values. See Section 21.9.3.35, “mysql_info()”. 


------------------------------------------------------------------------

I can just detect non utf charsets and abort the script and print out
the necessary conversion commands.



Final error is copying blobs:

..................archived_file_uploads
["id","archived_note_id","filename","content_type","bytesize","blob","created_at","updated_at"]
convert.hs: SqlError {seState = "22021", seNativeError = 7, seErrorMsg = "execute: PGRES_FATAL_ERROR: ERROR:  invalid byte sequence for encoding \"UTF8\": 0xe2e3cf\n"}
[choi@sparta mysql-ddl-parser]$ 

SqlColDesc {colType = SqlUnknownT "longblob", colSize = Nothing, colOctetLength = Nothing, colDecDigits = Nothing, colNullable = Just True},
SqlColDesc {colType = SqlTimestampT, colSize = Nothing, colOctetLength = Nothing, colDecDigits = Nothing, colNullable = Just True}


accesses
archived_file_uploads
/Users/choi/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/sequel-3.40.0/lib/sequel/adapters/postgres.rb:145:in `async_exec': PG::Error: ERROR:  invalid byte sequence for encoding "UTF8": 0xe2e3cf (Sequel::DatabaseError


       o   --compatible=name


mysqldump -uroot -t --compatible=postgresql mackey_production  > pgcompat.dump
 is useless 


[SqlInt32 5,SqlInt32 67,SqlByteString "0900b8c0837c86f8.pdf",SqlByteString "application/pdf",SqlInt32 368309,SqlByteString "%PDF-1.4\r%\226\227\207\211\r\n106 0 obj <</Linearized 1/L 350978/O 108/E 117787/N 17/T 348810/H [ 836 468]>>\rendobj\r            \r\nxref\r\n106 27\r\n0000000016 00000 n\r\n0000001304 00000 n\r\n0000001395 00000 n\r\n0000001603 00000 n\r\n0000001757 00000 n\r\n0000002343 00000 n\r\n000000



might have to do with lack of escaping by HDBC-postgresql


http://haskell.1045720.n5.nabble.com/HDBC-postgresql-bytestrings-and-embedded-NULLs-td3331598.html

Solution may be to use postgresql-simple on the insert side




http://hackage.haskell.org/packages/archive/postgresql-libpq/0.6/doc/html/Database-PostgreSQL-LibPQ.html



On my Dell XPS Ubuntu, Text columns cause UTF8 encoding errors: This is
their columnData:

("bio",SqlColDesc {colType = SqlBinaryT, colSize = Nothing, colOctetLength = Nothing, colDecDigits = Nothing, colNullable = Just True})


mysql text cols are SqlBinary

("title",SqlColDesc {colType = SqlVarCharT, colSize = Nothing, colOctetLength = Nothing, colDecDigits = Nothing, colNullable = Just True})
("start_date",SqlColDesc {colType = SqlTimestampT, colSize = Nothing, colOctetLength = Nothing, colDecDigits = Nothing, colNullable = Just True})
("body",SqlColDesc {colType = SqlBinaryT, colSize = Nothing, colOctetLength = Nothing, colDecDigits = Nothing, colNullable = Just True})
("created_at",SqlColDesc {colType = SqlTimestampT, colSize = Nothing, colOctetLength = Nothing, colDecDigits = Nothing, colNullable = Just True})

